## Hierarchical Clustering ###

##Business objective: clustering of data using unsupervised hierarical clustering method and draw interferance from clustered data.
#                       the data is all about auto insurance details of people with consumer id. 

#constraint: the is mixed data (obj and float).


import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

import pandas as pd
#import data file
df = pd.read_csv("D:\\360.digi\\Assignment\\Clustering-Hierarchical Clustering\\Dataset_Assignment Clustering\\AutoInsurance.csv")

df1 = df.drop(["Customer","Effective To Date","Policy Type"], axis=1)


##### exploratory data analysis ######
df1.dtypes
df1.describe()
df1.info()
df1.std()
df1.var()
# expect "number of complants" all numeric columns as to be normalize.

#checking data by calucalating skews and kurtosis.
import scipy.stats

scipy.stats.skew(df1["Customer Lifetime Value"])#skewness-3.031 #positive skew
plt.hist(df1["Customer Lifetime Value"])##data are not normally distributed

scipy.stats.skew(df1["Income"])#skewness-0.2868 #positive skew
plt.hist(df1["Income"])##data are not normally distributed

scipy.stats.skew(df1["Monthly Premium Auto"])#skewness-2.12 #positive skew
plt.hist(df1["Monthly Premium Auto"])##data are not normally distributed

scipy.stats.skew(df1["Months Since Last Claim"])#skewness-0.2785 #positive skew
plt.hist(df1["Months Since Last Claim"])#data are not normally distributed

scipy.stats.skew(df1["Months Since Policy Inception"])#skewness-0.04015
plt.hist(df1["Months Since Policy Inception"])#data uniformly distributed

scipy.stats.skew(df1["Number of Open Complaints"])#skewness-2.782 #positive skew
plt.hist(df1["Number of Open Complaints"])#data are not normally distributed

scipy.stats.skew(df1["Number of Policies"])#skewness-1.253 #positive skew
plt.hist(df1["Number of Policies"])#data are not normally distributed

scipy.stats.skew(df1["Total Claim Amount"])#skewness-1.714 #positive skew
plt.hist(df1["Total Claim Amount"]) #data are not normally distributed


scipy.stats.kurtosis(df1["Customer Lifetime Value"])#kurtosisness= 13.81 >3
scipy.stats.kurtosis(df1["Income"])#kurtosisness= -1.094 <3
scipy.stats.kurtosis(df1["Monthly Premium Auto"])#kurtosisness= 6.189 >3
scipy.stats.kurtosis(df1["Months Since Last Claim"])#kurtosis= -1.07  <3
scipy.stats.kurtosis(df1["Months Since Policy Inception"])#kurtosis= -1.13 <3
scipy.stats.kurtosis(df1["Number of Open Complaints"])#kurtosis= 7.744  >3
scipy.stats.kurtosis(df1["Number of Policies"])#kurtosis=0.362  <3
scipy.stats.kurtosis(df1["Total Claim Amount"])#kurtosis=5.975  >3


########  Data processing  ######

#####MissingValues#####

# using isnull() function   
df1.isnull()
#check null values in data set
df1.isnull().sum() #no null values


####Duplication_Typecasting#####
#Identify duplicates records in the data
duplicate = df.duplicated()
sum(duplicate) #no duplicates

####Outlier_Treatment#######

sns.boxplot(df1["Customer Lifetime Value"]);plt.title("Customer Lifetime Value");plt.show()# outliers
sns.boxplot(df1["Income"]);plt.title("Income");plt.show()##no outliers-
sns.boxplot(df1["Monthly Premium Auto"]);plt.title("Monthly Premium Auto");plt.show()# outliers
sns.boxplot(df1["Months Since Last Claim"]);plt.title("Months Since Last Claim");plt.show()#no outliers
sns.boxplot(df1["Months Since Policy Inception"]);plt.title("Months Since Policy Inception");plt.show()#no outliers
sns.boxplot(df1["Number of Open Complaints"]);plt.title("Number of Open Complaints");plt.show()## outliers
sns.boxplot(df1["Number of Policies"]);plt.title("Number of Policies");plt.show()# outliers
sns.boxplot(df1["Total Claim Amount"]);plt.title("Total Claim Amount");plt.show()# outliers
#the outliers seen in the plots are acceptable because data itself in that way so we cannot consider then has outliers.
#no outlier treatment required.


#####Dummy Variables#######
# Create dummy variables on categorcal columns
df_new = pd.get_dummies(df1,drop_first=True)

#?pd.get_dummies


######Standardization######

#standardization using function
#def function(i):
#    x=(i-i.mean())/(i.std())
#    return(x)
#df_norm=function(df_new.iloc[:,0:8])

from sklearn.preprocessing import normalize
data_scaled = pd.DataFrame(normalize(df_new.iloc[:,0:8]))
data_scaled.std()
data_scaled.var()

#### custom normalization #####
def normal(i):
    x=(i-i.min())/(i.max()-i.min())
    return (x)
  
df_norm=normal(df_new.iloc[:,0:8])
df_norm.std()
df_norm.var()


df_final=pd.concat([df_norm,df_new.iloc[:,8:]],axis=1)


####  unsupervise learning  #####

### Hierarchical clustering ####

# for creating dendrogram 
from scipy.cluster.hierarchy import linkage
import scipy.cluster.hierarchy as sch 

#here we calculating distance matrix and creating dendrogram
z = linkage(df_final, method = "complete", metric = "euclidean")
#?linkage
# Dendrogram
plt.figure(figsize=(15, 8));plt.title('Hierarchical Clustering Dendrogram');plt.xlabel('Index');plt.ylabel('Distance')
sch.dendrogram(z,leaf_rotation = 0,leaf_font_size = 10);plt.show()
#sch.dendrogram(z)

#from the plot we decide how many clusters are to be made 

# Now applying AgglomerativeClustering choosing 4 as clusters from the above dendrogram
from sklearn.cluster import AgglomerativeClustering
#agglomerative function is fitted or applied on dataset
h_complete = AgglomerativeClustering(n_clusters = 4, linkage = 'complete', affinity = "euclidean").fit(df_norm) 
#?Agglomerative

h_complete.labels_ #for that function we are taking labels
#from the function take labels
#converting into series
cluster_labels = pd.Series(h_complete.labels_)

df['clust'] = cluster_labels # creating a new column and assigning it to new column 

#change the postion of new column
df_clust = df.iloc[:, [24,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]]
df_clust.head()#displays only first five columns

# Aggregate mean of each cluster
mean_label=df_new.iloc[:,0:8].groupby(df.clust).mean()

# creating a csv file 
df_clust.to_csv("Autoinsurance_hier.csv", encoding = "utf-8") #understand format
mean_label.to_csv("mean_Autoinsurance_hier.csv", encoding = "utf-8")

import os
os.getcwd() #to see working directory

##Conclusion: As per the dendogram diagram the data clustered into four categories. The conclusions are drawn from mean_Autoinsurance_hier.csv.
#1. The clustering is done as per customer life time and total claim amount. so, those who have high life time have high claim amount independent of income.
#2. The remaining entities flows as per customer life time.
#3. Lot of information can be drawn from various analysis depending upon requirement.

